[["index.html", "FMISD13207 Cloud Security Technologies Introduction", " FMISD13207 Cloud Security Technologies Kstutis Daugla 2022-05-03 Introduction Information Security Fundamentals (information security problematics, classification and evolution of threats, identification, authentication, access control, security principals, strategies, models, taxonomies and antologies); Cryptography (simetric and public key cryptography, DES, AES, RSA. stream ciphers, cryptographic protocols, authentication, electronic signature, management of electronic identity); Network Security (routing, firewalls, VPN, web security, network perimeter protection, host-level protection, authentication technologies); Attacking Information Technology Systems (attack types, real-life case studies, intrusion detection, formal analysis techniques); Information Security Technologies (antivirus, IDS, host and perimeter protection systems, Honeypots); Implementing Effective Information Security Programs (legal, regulatory and privacy issues, security standarts, security best practices, security policy). The underlying concept of cloud computing was introduced way back in 1960s by John McCarthy in his book, The challenge of the Computer Utility. His opinion was that computation may someday be organized as a public utility. The rest became history and the majority of the software used now is running in the cloud seamlessly (Surbiryala and Rong 2019). The history of the cloud - image source https://itchronicles.com/ Cloud can solve a lot of problems nowadays - starting with reduced cost, enhanced security, and flexible approach (Srivastava and Khan 2018) up to sustainability (Parthasarathy and Kumar 2012) and accessibility around the world. Continuous Integration and Deployment (CI/CD) is easier than even treating now only the applications, but the whole infrastructure as code. This leads to enhanced productivity and cost optimization (Garg and Garg 2019). Is there anything revolutionary in the cloud offerings today? Definitely, no - people used these capabilities for ages. The only difference is the scale and popularity these days. Cloud services usually are grouped into three categories: SaaS (Software as a service) is a software distribution model in which a cloud provider hosts applications and makes them available to end-users over the internet PaaS (Platform as a service) is a complete development and deployment environment in the cloud, with resources that enable you to deliver everything from simple cloud-based apps to sophisticated, cloud-enabled enterprise applications IaaS (Infrastructure as a service) is a type of cloud computing service that offers essential compute, storage, and networking resources on-demand, on a pay-as-you-go basis IaaS vs PaaS vs SaaS - image source https://www.bigcommerce.com/blog/saas-vs-paas-vs-iaas/ However, despite the gain achieved from cloud computing, organizations are slow in fully accepting it due to security issues and challenges associated with it (Bairagi and Bang 2015). However, almost every cloud-ready company uses the public cloud (97%) to some extent leaving hybrid cloud setup the dominant one (78%). Companies rarely use public or private cloud alone (19% vs 2% respectively). According to Forbes, there are now 77 % of organizations, having one or some parts of their systems in the cloud. Cloud service providers follow a shared security responsibility model, which means that your security team retains some security duties as you migrate applications, data, containers, and workloads to the cloud, while the provider takes part, but not all, of the responsibility. Clearly defining your duties from those of your providers is critical for minimizing the risk of introducing vulnerabilities into your public, hybrid, or multi-cloud systems, as shown in the graph below. Research challenges and directions have been categories into 6 groups namely Security, Autonomic Resource Management (ARM), Cloud Adoption, Cloud development and benchmarking big data technologies and cloud computing and social clouds. Security is the most common challenge of cloud computing. This paper is the outcome of literature review of over 190 resources including books, journal and conference papers, research project reports and deliverables, European Commission roadmaps and calls for proposals, online weblogs, white papers, business reviews, company websites, profiles and offered services by cloud providers. (Keshavarzi, Haghighat, and Bohlouli 2020) One of our main findings is that hacking breaches are the most important in terms of leaked data and in terms of financial impact. We found that the most targeted type of organisation are medical organisations and BSOs since they possess the most sensitive personal data. Moreover, we observed that in the recent years the frequency hacking breaches on the two sectors has intensified, with an inter-arrival time of 4 and 7.5 days respectively. These findings implies taking serious actions to secure personal data especially for MED and BSO organization (Hammouchi et al. 2019) Worlds Biggest Data Breaches &amp; Hacks - image source https://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/ References "],["type-of-attacks.html", "Chapter 1 Type of attacks", " Chapter 1 Type of attacks The literature has examined a variety of security flaws(Humayun et al. 2020). To aid readers in comprehending some of the most prevalent cyber security weaknesses, the following are detailed: The Vulnerabilities that a Business can Experience - image source https://blog.ecosystm360.com/cyber-attacks-threats-risks/ Malware. For the previous decade, malware attacks have been the most serious cyber security danger to many enterprises (Lim and Lukito 2013). Malware The attacker uses malicious software to gain unauthorized access to computer systems by exploiting their security flaws. Malware is motivated by an extreme financial or political gain, which increases an attackers drive to compromise as many network devices as possible in order to accomplish their harmful goals. Viruses, worms, trojans, backdoors and adware are but a few examples that fall under the umbrella of malware (Mokoena and Zuva 2017). DDoS. Cyber security is comprised on three essential components: confidentiality, integrity, and availability. Denial of Service (DoS) attacks and their version, Distributed Denial of Service (DDoS), are conceivable threats that deplete system resources, rendering them inaccessible to authorized users, hence breaching one of the security componentsavailability. DoS attacks on networks are widespread and have the potential to be catastrophic. Numerous types of DoS attacks have been identified thus far, and the most of them are extremely efficient at disrupting network connectivity. Both IPv4 and IPv6 are quite vulnerable to these attacks (Tripathi and Mehtre 2013). The frequency and scale of attacks have escalated in recent years, from a few megabytes to hundreds of gigabytes. It is difficult to identify these attacks efficiently due to changes in attack patterns or new forms of attacks (Vanitha, UMA, and Mahidhar 2017). Phishing is a very effective approach of cybercrime that criminals use to fool consumers and steal critical data. Since the first phishing assault was disclosed in 1990, the attack vector has grown into a more sophisticated attack vector. At the moment, phishing is regarded to be one of the most prevalent forms of fraud on the Internet. Phishing attacks can result in significant losses for their victims, including sensitive data, identity theft, businesses, and state secrets. (Alkhalil et al. 2021) SQL Injection Attack (SQLIA) is one of the most terrifying risks to web applications. Input validation flaws were the cause of a SQL injection attack on the web. SQLIA is a harmful behavior that exploits invalid SQL statements in order to exploit data-driven applications. This vulnerability allows an attacker to exploit manipulated input to gain access to the applications back-end databases through the applications interaction with them. As a result, the attacker can acquire access to the database without obtaining genuine clearance by introducing, changing, or removing key information (Hlaing and Khaing 2020). Man-in-the-middle. An attack in which an outsider or third party infiltrates the space between two online users while both users are unaware. In this instance, the malware primarily monitors and has the capacity to modify the information classified exclusively to these two people. Generally, it is referred to as a protocol to refer to an unauthorized user within the system who has the ability to view and modify the systems data without leaving a trail for the systems existing users (Javeed and MohammedBadamasi 2020). Cross-site scripting is a serious issue in Web Applications. With more connected devices that use a variety of Web Applications for various tasks, the potential of XSS assaults grows. By exploiting XSS vulnerabilities in Web applications, hackers can steal victims session details or other sensitive information(Nagarjun and Ahamad 2020). Zero-day exploit continue to be a significant security concern to enterprises. When a vendor becomes aware of a zero-day vulnerability, releasing a fix in a timely manner becomes a priority due to the possibility of zero-day exploits. However, we continue to lack knowledge on the factors that influence the time it takes for such vulnerabilities to be patched. It was discovered that while IT companies are quick to release timely updates for zero-day vulnerabilities that affect multiple vendors, products, and versions, vulnerabilities that require privileges and compromise confidentiality are less likely to be patched on time. (Roumani 2021). BEC (Business Email Compromise) is a sophisticated email fraud scheme that targets firms who deal with international suppliers and frequently transfer payments via wire transfers. BEC attacks are designed to eliminate security protections by capitalizing on flaws in human behavior and decision-making (Agazzi 2020). References "],["data-breach---case-study.html", "Chapter 2 Data Breach - Case Study 2.1 Twitch 2.2 Phizer 2.3 Citybee", " Chapter 2 Data Breach - Case Study 2.1 Twitch Twitch is a global community that comes together each day to create multiplayer entertainment: unique, live, unpredictable experiences created by the shared interactions of millions. Acquired by Amazon in 2014. UpGuard Security Rating - image source https://www.upguard.com/ Twitch recently suffered a data breach that, according to security analysts, may have revealed extensive information about the platforms computer code, security vulnerabilities, and payments to content providers. According to the source, the file contained the history of Twitchs source code; proprietary software development kits; an unreleased competitor to Steam, an online games store; programs used by Twitch to test its own security vulnerabilities; and a list of the amount of money earned by each of the sites streamers since 2019 NY Times - A potentially disastrous data breach hits Twitch, the livestreaming site As another security proffessional indicates in medium.com - Thoughts on the Twitch Breach: the attackers did not gain access through a zero-day vulnerability or a supply chain assault. Something existed in a state that was not compliant with or anticipated of it. It appears that the problem was a misconfiguration as this statement says something was in error. The attackers did not get in due to a zero-day or supply chain attack. Something existed in a non-compliant or expected state. It sounds as though a server was running something on an Internet-facing port. However, the exposed data could have existed in an S3 bucket and all we know is that a server entered an undesirable state, which resulted in data disclosure. The misconfiguration could be as simple as a server being allowed excessive permissions and access, or as complex as a server being exposed to the Internet when it should have been in a private network. Many people host their data on AWS and might be thinking, If Amazon cant keep data secure on AWS, who can?. Assess that you have zero-trust networking and permissions that provide persons and applications only the access they require. In an enterprise of that size, source control systems, if that is what was impacted in this case, should never be exposed directly to the entire Internet. Small enterprises may do so temporarily to stay afloat, but the majority of source control solutions allow you to restrict access to specific IP ranges. 2.2 Phizer Pfizer Inc., the worlds largest pharmaceutical company, has experienced a massive data breach, with patient information discovered exposed on unprotected cloud storage. The exposed data was discovered in a Google Cloud storage bucket that had been misconfigured. Hundreds of discussions between Pfizers automated customer service software and consumers who used the companys prescription pharmaceuticals, including Lyrica, Chantix, Viagra, and cancer medicines Ibrance and Aromasin, were included in the data. Along with sensitive medical information, the transcripts included full names, home addresses, and email addresses, which hackers could use to conduct highly effective phishing attempts against victims: Pfizer suffers huge data breach on unsecured cloud storage. It is evident that data storage in the public cloud has become the standard, and businesses of all sizes now face complicated identity and data management challenges. Capital One demonstrated, and Pfizer has confirmed, that even with the largest teams, funds, and skill sets, the public cloud is highly difficult. When corporate organizations move at the speed of the cloud and innovate at a breakneck pace, errors and data exposure are inevitable if the necessary technologies are not in place. 2.3 Citybee Lithuanian police were investigating after 110,000 peoples personal information was exposed to an internet hacker website. CityBee, a car-sharing service, revealed that the breach compromised the records and information of thousands of its clients. euronews.com - Thousands of CityBee users have their personal data leaked online. As the attacker stated, CityBee was using a service provided by Microsoft called Azure Blob, which is used as storage of some sorts. Now Microsoft allows you to secure those blobs with authentication, which Citybee for some reason chose not to. He was able to search CityBee in a DNS record called CNAME which linked to their azure blob and other things like their website. Even a step-by-step instruction was provided by the hacker. Essentially, it was a BACPAC file named CitybeeProduction, which contained the metadata and data from the database. Citybee also used a very weak SHA-1 encryption algorithm for passwords without any salt added. On November 29, 2021, the Lithuanian data protection authority (VDAI) imposed a fine of EUR 110 000 on UAB Prime Leasing, which manages the short-term car rental platform CityBee (Company). VDAI found that it had failed to ensure the security of the processing of personal data. "],["public-cloud.html", "Chapter 3 Public Cloud 3.1 Public Cloud Security 3.2 Infrastructure as Code", " Chapter 3 Public Cloud 3.1 Public Cloud Security Cloud security is a critical matter. Most companies worry that highly sensitive data and intellectual property may be exposed through accidental leaks or due to increasingly sophisticated cyber attacks. Gartner predicts that through 2025, 99% of cloud security failures will be the customers fault. Moreover, having a solid cloud security stance helps organizations achieve other benefits, such as: Lower costs Reduced ongoing operational and administrative expenses Scalability Increased reliability and availability DevOps way of working Despite bringing many benefits, the cloud computing paradigm imposes serious concerns in terms of security and privacy, which are considered hurdles in the adoption of the cloud at a very large scale (Alghofaili et al. 2021). Security issues are depended on the cloud provider, service user, instance (Y. Sun et al. 2014), and the delivery model, PaaS, IaaS, and SaaS (X. Sun 2018). Data stored in the public cloud would face both outside attacks and inside attacks (Shi 2018). Data loss and leakage were the biggest security concern, with 44% of organizations seeing data loss as one of their top three focus areas. Two-thirds of organizations leave back doors open to attackers leading to an accidental exposure through misconfiguration. Security gaps in misconfigurations were exploited in 66% of attacks (Sophos 2020). How criminals are getting in, source - sophos.com Zero Trust security model enables securing cloud-native applications by encrypting all network communication, authenticating, and authorizing every request. The traditional trust management mechanisms represent a static trust relationship that falls deficit while meeting up the dynamic requirement of cloud services. (Mehraj and Banday 2020). In order to achieve a true zero-trust security model in the cloud, a combination of network and identity permission policies should be in place. The Zero Trust eXtended (ZTX) Ecosystem, Forrester Research, Inc., source - juniper.net To adequately address the modern dynamic threat environment requires(Agency 2021): Coordinated and aggressive system monitoring, system management, and defensive operations capabilities. Assuming all requests for critical resources and all network traffic may be malicious. Assuming all devices and infrastructure may be compromised. Accepting that all access approvals to critical resources incur a risk Some security recommendations for network security can be summarized as follows (Alghofaili et al. 2021): Secure communication techniques should be adopted: HTTPS for web applications, transmission channel must be encrypted by TLS Additional monitoring should be done (manual, automatic, ML based) Other public security services such as web application firewalls (WAF), virtual firewalls, virtual bastion machines, virtual host protection, and virtual database audit systems could be used 3.2 Infrastructure as Code There was a significant shift in development, deployment, and software application management during the past decade. The new approach is called Development Operations (DevOps) where Infrastructure as Code (IaC) plays a core role. While manual configurations in the Cloud context was a norm, nowadays it is fully automated using blueprints that are easily interpretable by machines. Moreover, IaC approach allows a faster and homogeneous configuration for the whole infrastructure. Usually, it is utilized by a specific declarative language (TerraForm, CloudFormation, Puppet) that allows users to describe the desired state of the infrastructure. This significantly reduces the time, complexity and helps to provision the infrastructure from the security, management, and costs perspectives. The whole idea behind IaC is simple - developers can write declarative statements that define the infrastructure necessary to run the code as opposed to writing a ticket/creating a task for administrators. Reproducibility and transparency come as a side effects. Infrastructure as Code Survey, source - thenewstack.io Terraform is one of the most popular ways to implement this pipeline, especially in a Cloud context. It is an open-source tool that lets you provision Google Cloud resources with declarative configuration files-resources such as virtual machines, containers, storage, and networking. It lets users manage Terraform configuration files in source control to maintain an ideal provisioning state for testing, production, and other environments. (Almuairfi and Alenezi 2020) Terraform example, source - cloud.google.com References "],["project-scope.html", "Chapter 4 Project Scope 4.1 Platform for anatical applications 4.2 Kubernetes and Docker 4.3 Shiny Server without security", " Chapter 4 Project Scope 4.1 Platform for anatical applications Shiny Server is a powerful open source back end application. It creates a web server optimized for hosting Shiny applications. Shiny Server enables you to host your apps in a controlled environment, such as within your corporation, ensuring that your Shiny app (and any data it requires) never leaves your control. The majority of Shiny apps are deployed within an organizations firewall, and because you can generally trust that your colleagues will not attempt to hack your app64, you dont need to worry about security. If, on the other hand, your app contains data that only a subset of your colleagues should have access to, or if you wish to make your app publicly accessible, you will need to invest some effort in security. When it comes to app security, there are two critical components to safeguard: your data and your compute resources. Shiny Server vs Shiny Server Pro While Shiny Server has some security features, the open source version is pretty basic: it does not have authentication, TLS/SSL (traffic is not encrypted), theres no defense to a denial of service attack and there is no way to distribute work for multiple R instances. However, some companies, e.g. Appsilon are specializing in enterprise Shiny dashboards using the open source version. We will attempt to establish a similar secure infrastructure in this project and to expose shiny applications to the world wide web as well. Shiny Analytics Hub - Architecture 4.2 Kubernetes and Docker The Open Source Shiny Server is a stand-alone application that runs on a single thread. While it is feasible to run many instances of shiny servers and divide work via load balancing, this is inefficient and far more expensive than using containerized applications. While it is possible to create the shiny-server container from scratch, there are a lot of pre-build docker images available including all the dependencies. Moreover, virtualized applications are highly preferred in general as opposed to IaaS approach (virtual machines), it makes sense to dig deeper in kubernetes and docker setup, regardless of the chosen service provider. Container evolution - kubernetes.io Kubernetes was founded by Ville Aikas, Joe Beda, Brendan Burns, and Craig McLuckie in collaboration with Google engineers Brian Grant and Tim Hockin in mid-2014. Googles Borg system heavily influenced kubernetes design (Verma et al. 2015) (Burns et al. 2016). While the Borg project was implemented entirely in C++, Kubernetes was rewritten in Go language. The main goal of kubernetes was to build on the capabilities of containers and provide significant gains in programmer productivity while easing the management of the system. Cloud Strategy - image source Flexera Kubernetes is the most popular container orchestration platform that enables users to create and run multiple containers in cloud environments. Kubernetes offers resource management to isolate the resource usage of containers on a host server because performance isolation is an important factor in terms of service quality. Environments running Kubernetes consist of the following key components: Kubernetes control planemanages Kubernetes clusters and the workloads running on them. Include components like the API Server, Scheduler, and Controller Manager. Kubernetes data planemachines that can run containerized workloads. Each node is managed by the kubelet, an agent that receives commands from the control plane. Podspods are the smallest unit provided by Kubernetes to manage containerized workloads. A pod typically includes several containers, which together form a functional unit or microservice. Persistent storagelocal storage on Kubernetes nodes is ephemeral, and is deleted when a pod shuts down. This can make it difficult to run stateful applications. Kubernetes provides the Persistent Volumes (PV) mechanism, allowing containerized applications to store data beyond the lifetime of a pod or node. 4.3 Shiny Server without security Computer networks are prone to attacks and it has a wide range of attacks associated with them. Cloud is not an exception and even holds more risk. It can be prone to Denial-of-service, Eavesdropping, Host Attacks, Password Guessing, Protocol-based, and Social Engineering attacks (Chopra 2016). As an experiment, the firewall was opened to the whole world and network activity was monitored for one week. While the activity in the Compute Instance (Shiny Server hosted on a Virtual Server) was marginal, the exposed Shiny Server instance on Google Kubernetes Cluster was scanned extensively. This could be due to the rules on how Google generates IP addresses for corresponding instances. Moreover, GKE was exposed on port 80 which is a standard HTTP port, while the standard port of shiny server (3838) was used for Compute Instance, which is not that common configuration. Incoming Requests While the majority of the requests came from the USA, applications from China and Russia also scanned our exposed application considerably. These scans also are not centralized but are rather done by individuals or companies which specialize in data mining and web crawling. Some requests are also received from Lithuania, CGates Internet Service Provider. Incoming Requets from different cities Some of the IPs were crossed check with a publicly available IP database. These IP addresses, especially from China and Russia, were already reported a number of times and are indicated as abusive. Blacklisted IPs - source https://www.abuseipdb.com/ The analysis proves that an incorrectly configured firewall poses one of the most significant security risks. Misconfigured applications could serve as a back door and is a low handing fruit for hackers - e.g. it is easy to run a port scan for a specific IP range and use a collection of scripts/exploits to check whether there are any holes in the application. If any sensitive data where General Data Protection Regulation is not applied (i.e. USA, China, Russia). References "],["security-considerations.html", "Chapter 5 Security considerations 5.1 Certificates and TLS 5.2 Single Sign On 5.3 Network", " Chapter 5 Security considerations 5.1 Certificates and TLS TLS (Transport Layer Security) encrypts data transmitted over the Internet, ensuring that snoopers and hackers cannot view what you transmit. This is especially important for private and sensitive information such as passwords, credit card numbers, and personal communications. TLS originated from Secure Socket Layers (SSL), which was originally created to secure web sessions in 1994 by Netscape Communications Corporation. SSL 1.0 was never released publicly, while SSL 2.0 was rapidly superseded by SSL 3.0, the foundation for TLS. Historically, data was transferred unencrypted over the Internet, and encryption was often applied piecemeal to critical information such as passwords or payment information. Without TLS, not only may sensitive information such as usernames, passwords, and personal information be easily gathered, but also surfing habits, e-mail conversations, online chats, and conference calls can be observed. By enabling TLS support in client and server apps, it ensures that data exchanged between them is encrypted using secure algorithms and is inaccessible to third parties. TLS is the most frequently used encryption protocol in the internet. It is composed of two protocols: the TLS Handshake Protocol, which is responsible for authentication and key establishment, and the TLS Record Protocol, which is responsible for the subsequent usage of those keys to secure bulk data. SSL/TLS protects the confidentiality and integrity of data-in-transit by utilizing both asymmetric and symmetric encryption. Asymmetric encryption is used to establish a secure connection between a client and a server, and symmetric encryption is used to exchange data within the secured connection. Asymmetric (Public Key) Cryptography use case as encryption - source https://www.twilio.com/blog/what-is-public-key-cryptography At a high level, TLS is split into two phases: A handshake phase where a secure communication is negotiated and created between two participants. A post-handshake phase where communications are encrypted between the two participants. A handshake and post-handshake communitcation TLS 1.2 and earlier versions perform the key exchange once both parties agree on the key exchange algorithm to use. This means they first agree on a method to utilize before exchanging public keys. To eliminate the first round of negotiation (one client message and one server message) in TLS 1.3, the client transmits a public key speculatively in the very first message (the Client Hello). If the client is unable to correctly forecast the servers key exchange mechanism, the client must send a new Client Hello with the right public key. TLS 1.3 contains numerous such enhancements that are critical for the web. Indeed, many people on the planet have insecure or poor connections, making it critical to restrict non-application communication to a bare minimum. Additionally, unlike prior versions of TLS, all key exchanges in TLS 1.3 are ephemeral. This means that for each new session, both the client and server generate new key pairs and then delete them immediately after the key exchange is complete (Wong 2021). https://freecontent.manning.com/how-does-tls-work/ Handshake: Negotiation. TLS is highly configurable. Both a client and a server can be configured to negotiate a range of SSL and TLS versions, as well as a menu of acceptable cryptographic algorithms. The negotiation phase of the handshake aims at finding common ground between the clients and the servers configurations, in order to securely connect the two peers. Key exchange. The whole point of the handshake is to perform a key exchange between the two participants. What key exchange algorithm to use? This is one of the things decided as part of the negotiation process. Authentication. It is trivial for a MITM attacker to impersonate any side of a key exchange. For this reason, key exchanges must be authenticated. (Your browser must have a way to make sure that it is talking to google.com and not your Internet service provider, for example.) Session Resumption. As browsers often connect to the same websites again and again, key exchanges can be costly and slow down a users experience. For this reason, mechanisms to fast-track secure sessions without redoing a key exchange are integrated into TLS. In TLS 1.3, to avoid that first negotiating round trip (one client message and one server message), the client speculatively sends a public key in the very first message (the Client Hello). If the client fails to predict the servers choice of key exchange algorithm then the client will have to send a new Client Hello containing the correct public key. For example: The client sends a TLS 1.3 Client Hello announcing that it can do either an X25519 or an X448 key exchange. It also sends an X25519 public key. The server does not support X25519, but does support X448. It sends a Hello Retry Request to the client announcing that it only supports X448. The client sends the same Client Hello but with an X448 public key instead. The handshake goes on. RSA and the Diffie-Hellman Key Exchange are the two most widely used encryption algorithms, each of which solves the same problem in a somewhat different way. In a nutshell, the Diffie Hellman technique generates a public and private key for each party to the transaction but shares only the public key. Unlike Diffie-Hellman, the RSA algorithm can be used for both digital signatures and symmetric key exchange, although it does need the prior exchange of a public key. RSA is based on the assumption that factoring really large integers is difficult (integer factorization). On the assumption that there is no efficient solution for integer factorization, it is thought that decrypting an RSA ciphertext completely is infeasible. A user of RSA generates and then publishes their public key, which is composed of two enormous prime integers and an auxiliary value. The primary variables must remain unidentified. While anyone can use the public key to encrypt a message, only someone familiar with the prime factors is likely to be able to decrypt it. RSA stands for Ron Rivest, Adi Shamir, and Leonard Adlemanthe three men who initially publicly defined the approach in 1977. Elliptic curve cryptography (ECC) is based on the algebraic structure of elliptic curves in finite fields. It is believed that computing the discrete logarithm of a random elliptic curve element in regard to a publicly known base point is impossible. In 1985, Neal Koblitz and Victor S. Miller independently pioneered the use of elliptic curves in encryption; ECC techniques gained widespread adoption in 2004. ECC has the advantage over RSA in that the key can be smaller, which improves performance and security. The disadvantage is that not all services and applications are compatible with ECC-based SSL Certificates. Following that, a CA can provide a signature for the website using the websites public key. Given that the CAs signature is typically valid for years, we refer to it as a long-term signing public key (as opposed to an ephemeral public key). More precisely, CAs do not sign public keys directly, but rather sign certificates. A certificate contains the long-term public key, as well as some additional critical metadata, such as the domain name of your web page if you are one. To reassure your browser that the server it is communicating with is truly google.com, the server provides a certificate chain as part of the TLS handshake. This certificate chain consists of the following: Its own (leaf) certificate, which includes the domain name google.com, Googles long-term signing public key, and a signature from a certificate authority. A chain of intermediate certificate authority (CA) certificates, beginning with the one that signed Googles certificate and ending with the root CA that signed the final intermediate CA. After establishing a handshake and generating symmetric keys, both the client and server can transfer encrypted application data to one another. Not only that, TLS prevents such communications from being replayed or reordered. 5.2 Single Sign On Authentication and trust mechanisms are needed by the user and provider alike. In this scenario, SSO could be a good starting point. The spam e-mail problem can be also mitigated in the cloud. While often used interchangeably, authentication and authorization represent fundamentally different functions. In simple terms, authentication is the process of verifying who a user is, while authorization is the process of verifying what they have access to. Authentication vs. Authorization - source www.okta.com OAuth 2.0: If youve ever signed up to a new application and agreed to let it automatically source new contacts via Facebook or your phone contacts, then youve likely used OAuth 2.0. This standard provides secure delegated access. That means an application can take actions or access resources from a server on behalf of the user, without them having to share their credentials. It does this by allowing the identity provider (IdP) to issue tokens to third-party applications with the users approval. OpenID Connect: If youve used your Google to sign in to applications like YouTube, or Facebook to log into an online shopping cart, then youre familiar with this authentication option. OpenID Connect is an open standard that organizations use to authenticate users. IdPs use this so that users can sign in to the IdP, and then access other websites and apps without having to log in or share their sign-in information. SAML: Youve more likely experienced SAML authentication in action in the work environment. For example, it enables you to log into your corporate intranet or IdP and then access numerous additional services, such as Salesforce, Box, or Workday, without having to re-enter your credentials. SAML is an XML-based standard for exchanging authentication and authorization data between IdPs and service providers to verify the users identity and permissions, then grant or deny their access to services. 5.2.1 SAML (Armando et al. 2008) which is specifically designed for SSO. Service Provider (Resource Server)  this is the web-server you are trying to access information on. Client  this is how the user is interacting with the Resource Server, like a web app being served through a web browser. Identity Provider (Authorization Server)  this is the server that owns the user identities and credentials. Its who the user actually authenticates with. SAML has one feature that OAuth2 lacks: the SAML token contains the user identity information (because of signing). With OAuth2, you dont get that out of the box, and instead, the Resource Server needs to make an additional round trip to validate the token with the Authorization Server. On the other hand, with OAuth2 you can invalidate an access token on the Authorization Server, and disable it from further access to the Resource Server. Both approaches have nice features and both will work for SSO. We have proved out both concepts in multiple languages and various kinds of applications. At the end of the day OAuth2 seems to be a better fit for our needs (since there isnt an existing SAML infrastructure in place to utilize). OAuth2 provides a simpler and more standardized solution which covers all of our current needs and avoids the use of workarounds for interoperability with native applications. SAML 2.0 Flow - source www.mutuallyhuman.com 5.2.2 OAuth2 Unlike SAML, OAuth 2.0 (henceforth OAuth2), is a specification whose ink has barely dried (circa late 2012). It has the benefit of being recent and takes into consideration how the world has changed in the past eight years. Mobile devices and native applications are prevalent today in ways that SAML could not anticipate in 2005. OAuth 2.0 Flow - source www.mutuallyhuman.com 5.3 Network References "],["tba-implementation-in-gcp.html", "Chapter 6 [TBA] Implementation in GCP 6.1 Conteinarization 6.2 Shiny App security features 6.3 Cloud Security", " Chapter 6 [TBA] Implementation in GCP TBA https://shinycloud.online/ https://login.shinycloud.online/ 6.1 Conteinarization FROM rocker/shiny # install R package dependencies RUN apt-get update &amp;&amp; apt-get install -y \\ libcurl4-openssl-dev libssl-dev \\ ## clean up &amp;&amp; apt-get clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/ \\ &amp;&amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds ## Install packages from CRAN RUN install2.r --error \\ -r &#39;http://cran.rstudio.com&#39; \\ googleAuthR COPY shiny-customized.config /etc/shiny-server/shiny-server.conf COPY client.json /srv/shiny-server/client.json COPY app.R /srv/shiny-server/app.R RUN chmod -R 775 /srv/shiny-server/ EXPOSE 8080 USER shiny CMD [&quot;/usr/bin/shiny-server&quot;] gcloud auth login --project {project id} PROJECTID=$(gcloud config get-value project) docker build . -t gcr.io/$PROJECTID/signin docker push gcr.io/$PROJECTID/signin 6.1.1 Cloud Run 6.1.2 GKE 6.2 Shiny App security features 6.2.1 Shiny server authentication 6.2.2 Shiny authorization 6.3 Cloud Security 6.3.1 Firewalls 6.3.2 DNS gcloud beta dns --project=$PROJECTID managed-zones create shinycloud-online --description=&quot;&quot; --dns-name=&quot;shinycloud.online.&quot; --visibility=&quot;public&quot; --dnssec-state=&quot;off&quot; --log-dns-queries 6.3.3 Certificates 6.3.4 Identity aware proxy 6.3.5 Audit "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
