[["index.html", "FMISD13207 Cloud Security Technologies Introduction 0.1 Cloud Computing 0.2 Data Breach - Case Study", " FMISD13207 Cloud Security Technologies Kęstutis Daugėla 2022-06-12 Introduction Topics from FMISD13207 Cloud Security Technologies module: Information Security Fundamentals (information security problematics, classification and evolution of threats, identification, authentication, access control, security principals, strategies, models, taxonomies and antologies); Cryptography (simetric and public key cryptography, DES, AES, RSA. stream ciphers, cryptographic protocols, authentication, electronic signature, management of electronic identity); Network Security (routing, firewalls, VPN, web security, network perimeter protection, host-level protection, authentication technologies); Attacking Information Technology Systems (attack types, real-life case studies, intrusion detection, formal analysis techniques); Information Security Technologies (antivirus, IDS, host and perimeter protection systems, Honeypots); Implementing Effective Information Security Programs (legal, regulatory and privacy issues, security standards, security best practices, security policy). 0.1 Cloud Computing The concept of cloud computing was introduced in 1960s by John McCarthy in his book, “The challenge of the Computer Utility”. His opinion was that “computation may someday be organized as a public utility.” The rest became history and the majority of the software used now is running in the cloud seamlessly (Surbiryala and Rong 2019). The history of the cloud - image source https://itchronicles.com/ Cloud can solve a lot of problems nowadays - starting with reduced cost, enhanced security, and flexible approach (Srivastava and Khan 2018) up to sustainability (Parthasarathy and Kumar 2012) and accessibility around the world. Continuous Integration and Deployment (CI/CD) is easier than even treating now only the applications, but the whole infrastructure as code. This leads to enhanced productivity and cost optimization (Garg and Garg 2019). Is there anything revolutionary in the cloud offerings today? Definitely, no - people used these capabilities for ages. The only difference is the scale and popularity these days. Cloud services usually are grouped into three categories: SaaS (Software as a service) is a software distribution model in which a cloud provider hosts applications and makes them available to end-users over the internet PaaS (Platform as a service) is a complete development and deployment environment in the cloud, with resources that enable you to deliver everything from simple cloud-based apps to sophisticated, cloud-enabled enterprise applications IaaS (Infrastructure as a service) is a type of cloud computing service that offers essential compute, storage, and networking resources on-demand, on a pay-as-you-go basis IaaS vs PaaS vs SaaS - image source https://www.bigcommerce.com/blog/saas-vs-paas-vs-iaas/ Despite the gain achieved from cloud computing, organizations are slow in fully accepting it due to security issues and challenges associated with it (Bairagi and Bang 2015). Almost every cloud-ready company uses the public cloud (97%) to some extent leaving hybrid cloud setup the dominant one (78%). Companies rarely use public or private cloud alone (19% vs 2% respectively). According to Forbes, there are now 77 % of organizations, having one or some parts of their systems in the cloud, despite the security concerns. 0.2 Data Breach - Case Study One of the main findings of the study (Hammouchi et al. 2019) was that the most targeted type of organisation are medical organisations and Business Services Organisations since they possess the most sensitive personal data. Moreover, it was observed that in the recent years the frequency hacking breaches on the two sectors has intensified, with an inter-arrival time of 4 and 7.5 days respectively. These findings implies taking serious actions to secure personal data especially for MED and BSO organization (Hammouchi et al. 2019). Another public data breach dataset of the biggest hacks confirms these findings as well. Attack spread between different business sectors are provided in the table below. Sector Attacks Number of records lost web 108 7009835665 government 44 922822573 health 43 174669929 finance 39 1162050100 retail 35 882555730 tech 23 1590382232 telecoms 22 837584000 app 21 553259000 transport 16 181626000 gaming 15 224490755 academic 11 5953000 misc 10 90780000 military 5 82227432 Even such tech companies as Facebook, Twitter, Amazon, Twitch were hacked successfully making a considerable damage on their reputation and their clients. World’s Biggest Data Breaches &amp; Hacks - image source https://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/ However, the classification for applied methods is not very precise and can be overlapping; for example, what is termed a “oops!” could be classified as bad security or vice versa. On the other hand, hacking methods are not described, and hence we shall explore attack types in greater detail in the second chapter. Hacks sorted by method In the section below we will discuss a few examples of data breaches that involve Cloud Computing. While these data breaches occurred using a different Public Cloud vendors (AWS, GCP and Azure), they have several things in common: zero-trust networking and permissions model that provide persons and applications only the access they require were not sufficient. These issues could be easily prevent by applying a proper permissions model and networking rules. 0.2.1 Twitch Twitch is a global community that comes together each day to create multiplayer entertainment: unique, live, unpredictable experiences created by the shared interactions of millions. Acquired by Amazon in 2014. UpGuard Security Rating - image source https://www.upguard.com/ Twitch recently suffered a data breach that, according to security analysts, may have revealed extensive information about the platform’s computer code, security vulnerabilities, and payments to content providers. According to the source, the file contained the history of Twitch’s source code; proprietary software development kits; an unreleased competitor to Steam, an online games store; programs used by Twitch to test its own security vulnerabilities; and a list of the amount of money earned by each of the site’s streamers since 2019 NY Times - A ‘potentially disastrous’ data breach hits Twitch, the livestreaming site As another security professional indicates in medium.com - Thoughts on the Twitch Breach: the attackers did not gain access through a zero-day vulnerability or a supply chain assault. Something existed in a state that was not compliant with or anticipated of it. It appears that the problem was a misconfiguration - it sounds as though a server was running something on an Internet-facing port. However, the exposed data could have existed in an S3 bucket and the only known aspect was that a server entered an undesirable state, which resulted in data disclosure. It could be either a server being allowed excessive permissions or a server being exposed to the Internet when it should have been in a private network. 0.2.2 Phizer Pfizer Inc., the world’s largest pharmaceutical company, has experienced a massive data breach, with patient information discovered exposed on unprotected cloud storage. The exposed data was discovered in a Google Cloud storage bucket that had been misconfigured. Hundreds of discussions between Pfizer’s automated customer service software and consumers who used the company’s prescription pharmaceuticals, including Lyrica, Chantix, Viagra, and cancer medicines Ibrance and Aromasin, were included in the data. Along with sensitive medical information, the transcripts included full names, home addresses, and email addresses, which hackers could use to conduct highly effective phishing attempts against victims: Pfizer suffers huge data breach on unsecured cloud storage. It is evident that data storage in the public cloud has become the standard, and businesses of all sizes now face complicated identity and data management challenges. Capital One demonstrated, and Pfizer has confirmed, that even with the largest teams, funds, and skill sets, the public cloud security is highly difficult. When corporate organizations move at the speed of the cloud and innovate at a breakneck pace, errors and data exposure are inevitable if the necessary technologies are not in place. 0.2.3 Citybee Lithuanian police were investigating after 110,000 people’s personal information was exposed to an internet hacker website. CityBee, a car-sharing service, revealed that the breach compromised the records and information of thousands of its clients. euronews.com - Thousands of CityBee users have their personal data leaked online. As the attacker stated, “CityBee was using a service provided by Microsoft called Azure Blob, which is used as storage of some sorts. Now Microsoft allows you to secure those blobs with authentication, which Citybee for some reason chose not to.” He was able to search CityBee in a DNS record called CNAME which linked to their azure blob and other things like their website. Even a step-by-step instruction was provided by the hacker. Essentially, it was a BACPAC file named CitybeeProduction, which contained the metadata and data from the database. Citybee also used a very weak SHA-1 encryption algorithm for passwords without any salt added. On November 29, 2021, the Lithuanian data protection authority (VDAI) imposed a fine of EUR 110 000 on UAB Prime Leasing, which manages the short-term car rental platform CityBee. VDAI found that it had failed to ensure the security of the processing of personal data. References "],["cloud-cybersecurity-management.html", "Chapter 1 Cloud Cybersecurity Management 1.1 Cloud Security Overview 1.2 Infrastructure as Code 1.3 Type of threats 1.4 Vulnerabilities 1.5 Risks", " Chapter 1 Cloud Cybersecurity Management Cloud security is a critical matter. Most companies worry that highly sensitive data and intellectual property may be exposed through accidental leaks or due to increasingly sophisticated cyber attacks. Gartner predicts that through 2025, 99% of cloud security failures will be the customer’s fault. Capturing, keeping, and utilizing sensitive data is necessary for the majority of companies, yet holding and accessing it imposes the duty to secure it. Threat is frequently confused with or used interchangeably with “risk” and “vulnerability.” In cybersecurity, however, it is essential to distinguish between threat, vulnerability, and risk. A threat exploits a weakness and can cause damage or destruction to an asset. A vulnerability is a flaw in your hardware, software, or operations while risk refers to the possibility of assets being lost, damaged, or destroyed. 1.1 Cloud Security Overview Cloud service providers follow a shared security responsibility model, which means that your security team retains some security duties as you migrate applications, data, containers, and workloads to the cloud, while the provider takes part, but not all, of the responsibility. Clearly defining your duties from those of your providers is critical for minimizing the risk of introducing vulnerabilities into your public, hybrid, or multi-cloud systems, as shown in the graph below. Shared responsibility in the cloud - image source www.microsoft.com Cloud challenges and directions have been categorized into 6 groups: Security, Autonomic Resource Management (ARM), Cloud Adoption, Cloud development, benchmarking big data technologies, cloud computing and social clouds. Security is the most common challenge of cloud computing as the paper indicates. The reseach is the outcome of literature review of over 190 resources including books, journal and conference papers, research project reports and deliverables, European Commission roadmaps and calls for proposals, online weblogs, white papers, business reviews, company websites, profiles and offered services by cloud providers. (Keshavarzi, Haghighat, and Bohlouli 2020) Having a solid cloud security stance helps organizations achieve other benefits, such as: Lower costs Reduced ongoing operational and administrative expenses Scalability Increased reliability and availability DevOps way of working Despite bringing many benefits, the cloud computing paradigm imposes serious concerns in terms of security and privacy, which are considered hurdles in the adoption of the cloud at a very large scale (Alghofaili et al. 2021). Security issues are depended on the cloud provider, service user, instance (Y. Sun et al. 2014), and the delivery model, PaaS, IaaS, and SaaS (X. Sun 2018). Data stored in the public cloud would face both outside attacks and inside attacks (Shi 2018). Data loss and leakage were the biggest security concern, with 44% of organizations seeing data loss as one of their top three focus areas. Two-thirds of organizations leave back doors open to attackers leading to an accidental exposure through misconfiguration. Security gaps in misconfigurations were exploited in 66% of attacks (Sophos 2020). How criminals are getting in, source - sophos.com Zero Trust security model enables securing cloud-native applications by encrypting all network communication, authenticating, and authorizing every request. The traditional trust management mechanisms represent a static trust relationship that falls deficit while meeting up the dynamic requirement of cloud services. (Mehraj and Banday 2020). In order to achieve a true zero-trust security model in the cloud, a combination of network and identity permission policies should be in place. The Zero Trust eXtended (ZTX) Ecosystem, Forrester Research, Inc., source - juniper.net To adequately address the modern dynamic threat environment requires(Agency 2021): Coordinated and aggressive system monitoring, system management, and defensive operations capabilities. Assuming all requests for critical resources and all network traffic may be malicious. Assuming all devices and infrastructure may be compromised. Accepting that all access approvals to critical resources incur a risk Some security recommendations for network security can be summarized as follows (Alghofaili et al. 2021): Secure communication techniques should be adopted: HTTPS for web applications, transmission channel must be encrypted by TLS Additional monitoring should be done (manual, automatic, ML based) Other public security services such as web application firewalls (WAF), virtual firewalls, virtual bastion machines, virtual host protection, and virtual database audit systems could be used 1.2 Infrastructure as Code There was a significant shift in development, deployment, and software application management during the past decade. The new approach is called Development Operations (DevOps) where Infrastructure as Code (IaC) plays a core role. While manual configurations in the Cloud context was a norm, nowadays it is fully automated using blueprints that are easily interpretable by machines. Moreover, IaC approach allows a faster and homogeneous configuration for the whole infrastructure. Usually, it is utilized by a specific declarative language (TerraForm, CloudFormation, Puppet) that allows users to describe the desired state of the infrastructure. This significantly reduces the time, complexity and helps to provision the infrastructure from the security, management, and costs perspectives. The whole idea behind IaC is simple - developers can write declarative statements that define the infrastructure necessary to run the code as opposed to writing a ticket/creating a task for administrators. Reproducibility and transparency come as a side effects. Infrastructure as Code Survey, source - thenewstack.io Terraform is one of the most popular ways to implement this pipeline, especially in a Cloud context. It is an open-source tool that lets you provision Google Cloud resources with declarative configuration files-resources such as virtual machines, containers, storage, and networking. It lets users manage Terraform configuration files in source control to maintain an ideal provisioning state for testing, production, and other environments. (Almuairfi and Alenezi 2020) Terraform example, source - cloud.google.com 1.3 Type of threats The literature has examined a variety of security threats (Humayun et al. 2020). To aid in comprehending some of the most prevalent cyber security weaknesses, the following are detailed: The threats that a Business can Experience - image source https://blog.ecosystm360.com/cyber-attacks-threats-risks/ Malware. For the previous decade, malware attacks have been the most serious cyber security danger to many enterprises (Lim and Lukito 2013). Malware The attacker uses malicious software to gain unauthorized access to computer systems by exploiting their security flaws. Malware is motivated by an extreme financial or political gain, which increases an attacker’s drive to compromise as many network devices as possible in order to accomplish their harmful goals. Viruses, worms, trojans, backdoors and adware are but a few examples that fall under the umbrella of malware (Mokoena and Zuva 2017). DDoS. Cyber security is comprised on three essential components: confidentiality, integrity, and availability. Denial of Service (DoS) attacks and their version, Distributed Denial of Service (DDoS), are conceivable threats that deplete system resources, rendering them inaccessible to authorized users, hence breaching one of the security components—availability. DoS attacks on networks are widespread and have the potential to be catastrophic. Numerous types of DoS attacks have been identified thus far, and the most of them are extremely efficient at disrupting network connectivity. Both IPv4 and IPv6 are quite vulnerable to these attacks (Tripathi and Mehtre 2013). The frequency and scale of attacks have escalated in recent years, from a few megabytes to hundreds of gigabytes. It is difficult to identify these attacks efficiently due to changes in attack patterns or new forms of attacks (Vanitha, UMA, and Mahidhar 2017). Phishing is a very effective approach of cybercrime that criminals use to fool consumers and steal critical data. Since the first phishing assault was disclosed in 1990, the attack vector has grown into a more sophisticated attack vector. At the moment, phishing is regarded to be one of the most prevalent forms of fraud on the Internet. Phishing attacks can result in significant losses for their victims, including sensitive data, identity theft, businesses, and state secrets. (Alkhalil et al. 2021) SQL Injection Attack (SQLIA) is one of the most terrifying risks to web applications. Input validation flaws were the cause of a SQL injection attack on the web. SQLIA is a harmful behavior that exploits invalid SQL statements in order to exploit data-driven applications. This vulnerability allows an attacker to exploit manipulated input to gain access to the application’s back-end databases through the application’s interaction with them. As a result, the attacker can acquire access to the database without obtaining genuine clearance by introducing, changing, or removing key information (Hlaing and Khaing 2020). Man-in-the-middle. An attack in which an outsider or third party infiltrates the space between two online users while both users are unaware. In this instance, the malware primarily monitors and has the capacity to modify the information classified exclusively to these two people. Generally, it is referred to as a protocol to refer to an unauthorized user within the system who has the ability to view and modify the system’s data without leaving a trail for the system’s existing users (Javeed and MohammedBadamasi 2020). Cross-site scripting is a serious issue in Web Applications. With more connected devices that use a variety of Web Applications for various tasks, the potential of XSS assaults grows. By exploiting XSS vulnerabilities in Web applications, hackers can steal victims’ session details or other sensitive information(Nagarjun and Ahamad 2020). Zero-day exploit continue to be a significant security concern to enterprises. When a vendor becomes aware of a zero-day vulnerability, releasing a fix in a timely manner becomes a priority due to the possibility of zero-day exploits. However, we continue to lack knowledge on the factors that influence the time it takes for such vulnerabilities to be patched. It was discovered that while IT companies are quick to release timely updates for zero-day vulnerabilities that affect multiple vendors, products, and versions, vulnerabilities that require privileges and compromise confidentiality are less likely to be patched on time. (Roumani 2021). BEC (Business Email Compromise) is a sophisticated email fraud scheme that targets firms who deal with international suppliers and frequently transfer payments via wire transfers. BEC attacks are designed to eliminate security protections by capitalizing on flaws in human behavior and decision-making (Agazzi 2020). 1.4 Vulnerabilities Vulnerabilities are weak points in the environment and assets, which expose you to possible threats and heightened risk. Unfortunately, a company can have thousands, and frequently millions, of vulnerabilities. It is not possible to remediate all of them, given that most businesses can only patch one out of every 10 vulnerabilities. However, the good news is that just 2 to 5 percent of vulnerabilities are likely to be exploited. Even fewer of these vulnerabilities are likely to constitute a real threat to a company, as many of them may not be actively exploited in the industry. Depending on the cloud service model, vulnerabilities might differ, as described in the table below. Layerwise threats vulnerabilities and solutions (Shaikh and Meshram 2021) 1.5 Risks Customers are supplied cloud services, however they are dissatisfied owing to vulnerabilities and risks in the distributed cloud environment. The SaaS is vulnerable to assaults such as denial of service and consumer data manipulation. The solution mechanisms for SAAS are trace back filters and Web application scanners. IaaS is susceptible to threats such as VM escape and malicious VM formation, for which there are solutions such as hypersafe, TCCP, and mirage. The attacker can also conduct attacks on PaaS, such as account or service hijacking and data leakage, using solution mechanisms such as identity and access management guidelines, dynamic credential and FRS approaches, digital signatures, and homogeneous encryption. Although many individuals develop assault protection techniques, there is no silver bullet to defend the cloud against ever-changing technology, intelligent attackers, and incompetent coders (Shaikh and Meshram 2021). References "],["paper-review.html", "Chapter 2 Paper review 2.1 Security Challenges in Different Network Layers 2.2 Anomaly detection for Cloud Applications", " Chapter 2 Paper review 2.1 Security Challenges in Different Network Layers A Comprehensive Survey on Security Challenges in Different Network Layers in Cloud Computing (Jangjou and Sohrabi 2022) Since cloud customers have shifted the security risks associated with their cloud-deployed systems and data to the cloud service provider. The disadvantages of transferring risks to the cloud service provider include unauthorized access to cloud users data. As a result of these security difficulties and other non-technical obstacles, such as cloud service providers mistrust of cloud customers, cloud service applicants are hesitant to adopt cloud services immediately. Using the five layers of computer networks, cloud computing deployment patterns, and the client and server layers services, the purpose of this paper was to investigate, analyze, and classify the security issues, vulnerabilities, and threats of cloud computing. Several solutions and preventative measures for establishing long-term security in the cloud computing environment were presented in this paper as long with a complete analysis of cloud computing, its architecture, services, and implementation techniques. In accordance with the recommended classification, security issues were also identified and categorized. Security requirements and preventive measures were proposed to assure the safety of user data against a variety of threats and reduce the risk. This article discusses how security responsibilities varies between three main service models (SAAS, PAAS, IAAS) and how responsibility shifts between the vendor and the customer. While in SaaS case the user does not have control of cloud infrastructure, the whole network must meet network security and availability demands in order to prevent unauthorized users and DooS attacks. In PAAS case, user have more control on the application level, but still lacks some networking and storage configuration capabilities. IaaS puts the user in control of OS, network, storage and other cloud resources. In terms of the deployment models, public cloud poses the most risk while being the cheapest option in terms of the infrastructure costs and flexibility. Network security layers were decoupled into five sections: perimeter, network, host, application, data. The perimeter layer connects user to the cloud environment, therefore it is prone to external threats. Solutions for that are Web Application Firewall and implemented Intrusion Detection System. Network layer, however, are mostly facing misconfiguration issues (e.g. opened ports, security patches not implemented, insecure protocols) and these security holes can enable users to reach the host level. It is important to isolate servers and storage logically in order to secure the network layer. Proper logging is also essential for detecting malicious behavior. Host level is susceptible to existing vulnerabilities, depending on the software hosted by the VM. Also it mandatory to ensure proper authentication mechanisms to a VM (e.g. LDAP, AD, SSO). The same part is applicable for the application level: bypassing the authentication or brute forcing the way are one of the most common attacks. On the last data layer, it is important to solve both - the communication and the storage security challenges. The full list of security challenges of different layers of the computer network are provided in the table below together with the probablities and the impact. Classifcation of security challenges of different layers of computer network (Jangjou and Sohrabi 2022) 2.2 Anomaly detection for Cloud Applications Machine Learning for Cloud Security: A Systematic Review (Nassif et al. 2021) Utilizing Machine Learning is one method for securing Cloud. ML approaches have been applied in a variety of ways to avoid or detect Cloud threats and security vulnerabilities. This study categorizes 63 relevant studies and the results of the Systematic Literature Review into three primary research areas: Cloud security risks, machine learning approaches, and model performance. Eleven Cloud security areas have been identified, including anomaly detection, attack detection, privacy preservation, security, vulnerability detection, data confidentiality, data privacy, DDoS, DoS, and intrusion detection. The most often analyzed topics are DDoS and data privacy, with a 16 percent and 14 percent usage frequency, respectively. Cloud security area found from collected research papers (Nassif et al. 2021) The primary role of Intrusion Detection Systems, which requires high accuracy, low false alarm rates, and the ability to predict alarms based on positive or real alarms when an intrusion occurs and false positive or false alarms in the event of a failure. Papers that discusses ID in cloud (Nassif et al. 2021) Anomaly detection is crucial because data anomalies are essential and frequently crucial information that can be utilized in a wide variety of applications. Some ML techniques are hybrid while others are standalone. SVM is the most prominent ML technique employed in hybrid and standalone models. Sixty percent of the studies compared their models to other ML models to obtain the best rating, either to demonstrate their model’s correctness or to further enhance it. ML models used in each research paper collected (Nassif et al. 2021) Some of the key finding of the authors were regarding lack of deep learning utilization and old datasets being used for model evaluation (e.g. KDD, KDD CUP 99). References "],["project-scope.html", "Chapter 3 Project Scope 3.1 Platform for anatical applications 3.2 Kubernetes and Docker 3.3 Shiny Server Security Experiment 3.4 Security considerations", " Chapter 3 Project Scope 3.1 Platform for anatical applications Shiny Server is a powerful open source back end application. It creates a web server optimized for hosting Shiny applications. Shiny Server enables you to host your apps in a controlled environment, such as within an corporation, ensuring that your Shiny app (and any data it requires) never leaves your control. The majority of Shiny apps are deployed within an organization’s firewall. If, on the other hand, an app contains data that only a subset of users should have access to, the security configuration in shiny server is not straightforward. When it comes to app security, there are two critical components to safeguard: your data and your compute resources. Shiny Server vs Shiny Server Pro While Shiny Server has some security features, the open source version is pretty basic: it does not have authentication, no TLS/SSL (traffic is not encrypted), there’s no defense to a denial of service attack and there is no way to distribute work for multiple R instances. However, some companies, e.g. Appsilon are specializing in enterprise Shiny dashboards using the open source version. We will attempt to establish a similar secure infrastructure in this project and to expose shiny applications to the world wide web as well. Shiny Analytics Hub - Architecture 3.2 Kubernetes and Docker The Open Source Shiny Server is a stand-alone application that runs on a single thread. It is feasible to run many instances of shiny servers and divide work via load balancing, but this is inefficient and far more expensive than using containerized applications. While it is possible to create the shiny-server container from scratch, there are a lot of pre-build docker images available including all the dependencies. Moreover, virtualized applications are highly preferred in general as opposed to IaaS approach (virtual machines), therefore it makes sense to dig deeper in kubernetes and docker setup, regardless of the chosen service provider. Container evolution - kubernetes.io Kubernetes was founded by Ville Aikas, Joe Beda, Brendan Burns, and Craig McLuckie in collaboration with Google engineers Brian Grant and Tim Hockin in mid-2014. Google’s Borg system heavily influenced kubernetes design (Verma et al. 2015) (Burns et al. 2016). While the Borg project was implemented entirely in C++, Kubernetes was rewritten in Go language. The main goal of kubernetes was to build on the capabilities of containers and provide significant gains in programmer productivity while easing the management of the system. Cloud Strategy - image source Flexera Kubernetes is the most popular container orchestration platform that enables users to create and run multiple containers in cloud environments. Kubernetes offers resource management to isolate the resource usage of containers on a host server because performance isolation is an important factor in terms of service quality. Environments running Kubernetes consist of the following key components: Kubernetes control plane—manages Kubernetes clusters and the workloads running on them. Include components like the API Server, Scheduler, and Controller Manager. Kubernetes data plane—machines that can run containerized workloads. Each node is managed by the kubelet, an agent that receives commands from the control plane. Pods—pods are the smallest unit provided by Kubernetes to manage containerized workloads. A pod typically includes several containers, which together form a functional unit or microservice. Persistent storage—local storage on Kubernetes nodes is ephemeral, and is deleted when a pod shuts down. This can make it difficult to run stateful applications. Kubernetes provides the Persistent Volumes (PV) mechanism, allowing containerized applications to store data beyond the lifetime of a pod or node. 3.3 Shiny Server Security Experiment Computer networks are prone to attacks and it has a wide range of attacks associated with them. Cloud is not an exception and even holds more risk. It can be prone to Denial-of-service, Eavesdropping, Host Attacks, Password Guessing, Protocol-based, and Social Engineering attacks (Chopra 2016). As an experiment, the firewall was opened to the whole world and network activity was monitored for one week. While the activity in the Compute Instance (Shiny Server hosted on a Virtual Server) was marginal, the exposed Shiny Server instance on Google Kubernetes Cluster was scanned extensively. This could be due to the rules on how Google generates IP addresses for corresponding instances. Moreover, GKE was exposed on port 80 which is a standard HTTP port, while the standard port of shiny server (3838) was used for Compute Instance, which is not that common configuration. Incoming Requests The majority of the requests came from the USA, but applications from China and Russia also scanned the exposed application considerably. These scans also are not centralized but are rather done by individuals or companies which specialize in data mining and web crawling. Some requests are also received from Lithuania, CGates Internet Service Provider. Incoming Requets from different cities Some of the IPs were crossed check with a publicly available IP database. These IP addresses, especially from China and Russia, were already reported a number of times and are indicated as abusive. Blacklisted IPs - source https://www.abuseipdb.com/ The analysis proves that an incorrectly configured firewall poses one of the most significant security risks. Misconfigured applications could serve as a back door and is a low handing fruit for hackers - e.g. it is easy to run a port scan for a specific IP range and use a collection of scripts/exploits to check whether there are any holes in the application. If any sensitive data where General Data Protection Regulation is not applied (i.e. USA, China, Russia). 3.4 Security considerations 3.4.1 Data Encryption TLS (Transport Layer Security) encrypts data transmitted over the Internet, ensuring that snoopers and hackers cannot view what you transmit. This is especially important for private and sensitive information such as passwords, credit card numbers, and personal communications. TLS originated from Secure Socket Layers (SSL), which was originally created to secure web sessions in 1994 by Netscape Communications Corporation. SSL 1.0 was never released publicly, while SSL 2.0 was rapidly superseded by SSL 3.0, the foundation for TLS. Historically, data was transferred unencrypted over the Internet, and encryption was often applied piecemeal to critical information such as passwords or payment information. Without TLS, not only may sensitive information such as usernames, passwords, and personal information be easily gathered, but also surfing habits, e-mail conversations, online chats, and conference calls can be observed. By enabling TLS support in client and server apps, it ensures that data exchanged between them is encrypted using secure algorithms and is inaccessible to third parties. TLS is the most frequently used encryption protocol in the internet. It is composed of two protocols: the TLS Handshake Protocol, which is responsible for authentication and key establishment, and the TLS Record Protocol, which is responsible for the subsequent usage of those keys to secure bulk data. SSL/TLS protects the confidentiality and integrity of data-in-transit by utilizing both asymmetric and symmetric encryption. Asymmetric encryption is used to establish a secure connection between a client and a server, and symmetric encryption is used to exchange data within the secured connection. Asymmetric (Public Key) Cryptography use case as encryption - source https://www.twilio.com/blog/what-is-public-key-cryptography At a high level, TLS is split into two phases: A handshake phase where a secure communication is negotiated and created between two participants. A post-handshake phase where communications are encrypted between the two participants. A handshake and post-handshake communitcation TLS 1.2 and earlier versions perform the key exchange once both parties agree on the key exchange algorithm to use. This means they first agree on a method to utilize before exchanging public keys. Asymmetric and symmetric within TLS - source https://blog.bytebytego.com/ To eliminate the first round of negotiation (one client message and one server message) in TLS 1.3, the client transmits a public key speculatively in the very first message (the Client Hello). If the client is unable to correctly forecast the server’s key exchange mechanism, the client must send a new Client Hello with the right public key. TLS 1.3 contains numerous such enhancements that are critical for the web. Indeed, many people have insecure or poor connections, making it critical to restrict non-application communication to a bare minimum. Additionally, unlike prior versions of TLS, all key exchanges in TLS 1.3 are ephemeral. This means that for each new session, both the client and server generate new key pairs and then delete them immediately after the key exchange is complete (Wong 2021). TLS performance comparison - https://www.a10networks.com The handshake itself contains of these following steps: Negotiation. TLS is highly configurable. Both a client and a server can be configured to negotiate a range of SSL and TLS versions, as well as a menu of acceptable cryptographic algorithms. The negotiation phase of the handshake aims at finding common ground between the client’s and the server’s configurations, in order to securely connect the two peers. Key exchange. The whole point of the handshake is to perform a key exchange between the two participants. What key exchange algorithm to use? This is one of the things decided as part of the negotiation process. Authentication. It is trivial for a MITM attacker to impersonate any side of a key exchange. For this reason, key exchanges must be authenticated. (Your browser must have a way to make sure that it is talking to google.com and not your Internet service provider, for example.) Session Resumption. As browsers often connect to the same websites again and again, key exchanges can be costly and slow down a user’s experience. For this reason, mechanisms to fast-track secure sessions without redoing a key exchange are integrated into TLS. An example of TLS 1.3 a negotiation could be: * The client sends a TLS 1.3 Client Hello announcing that it can do either an * X25519 or an X448 key exchange together with a X25519 public key. * The server does not support X25519, but does support X448. * It sends a Hello Retry Request to the client announcing that it only supports X448. * The client sends the same Client Hello but with an X448 public key instead. * The handshake goes on. RSA and the Diffie-Hellman Key Exchange are the two most widely used encryption algorithms, each of which solves the same problem in a somewhat different way. In a nutshell, the Diffie Hellman technique generates a public and private key for each party to the transaction but shares only the public key. Unlike Diffie-Hellman, the RSA algorithm can be used for both digital signatures and symmetric key exchange, although it does need the prior exchange of a public key. RSA stands for Ron Rivest, Adi Shamir, and Leonard Adleman—the three men who initially publicly defined the approach in 1977. RSA is based on the assumption that factoring really large integers is difficult (integer factorization). On the assumption that there is no efficient solution for integer factorization, it is thought that decrypting an RSA ciphertext completely is infeasible. A user of RSA generates and then publishes their public key, which is composed of two enormous prime integers and an auxiliary value. The primary variables must remain unidentified. While anyone can use the public key to encrypt a message, only someone familiar with the prime factors is likely to be able to decrypt it. The browser, on the other hand, verify that the handshake is happening between the client and specific website using the Public Key Infrastructure. Firstly, browsers must trust a set of root public keys - Certificate Authorities (CAs). Typically, browsers will either utilize a hardcoded list of trusted public keys or will request them from the operating system. Websites wishing to utilize HTTPS must be able to receive certification from one of these CAs (a signature of their signing public key). In order to do this, a website owner must prove to a CA that they own a specific domain. CAs do not directly sign public keys but rather sign certificates. A certificate contains both the long-term public key and some crucial metadata, such as the domain name of your web page, if you have one. To ensure that the server connecting with a browser is genuine, the server includes a certificate chain in the TLS handshake. This certificate chain is sent in a Certificate TLS message by the server, and by the client as well if the client has been asked to authenticate. The server sends this certificate chain in a Certificate TLS message, as does the client if the client is prompted to authenticate. Following the establishment of a handshake and the generation of symmetric keys, both the client and server can exchange encrypted application data. Additionally, TLS prevents reordering of such communications. 3.4.2 Single Sign On Authentication and trust mechanisms are needed by the user and provider alike. In this scenario, SSO could be a good starting point. The spam e-mail problem can be also mitigated in the cloud. Despite their frequent interchangeability, authentication and permission serve fundamentally distinct purposes. Authentication is the process of validating a user’s identity, whereas authorization is the process of checking the user’s access rights. As a general recommendation, the SSO (Single Sign-on) and CAS (Central Authentication Service) should be also activated to make a user to access various software services through a single active user account. (Jangjou and Sohrabi 2022) SAML and OAUTH2 are two of the most popular Single Sign-On implementation methods. 3.4.2.1 SAML SAML is an abbreviation for Security Assertion Markup Language (SAML). Its principal function in terms of online security is to permit access to different web applications with a single set of login credentials. It operates by exchanging authentication data between two parties, often an identity provider (idP) and a web application. Web applications utilize SAML to send authentication data between two parties - the identity provider (IdP) and the service provider - in an XML-based format (SP). SAML was developed by the technology sector to ease the authentication procedure for users requiring access to various, independent web apps across domains. Single sign-on (SSO) was possible before SAML, but it relied on cookies that were only valid within the same domain. It accomplishes this by centralizing user authentication through an identity provider. Then, Web apps can utilize SAML via the identity provider to provide their users access. This SAML authentication method eliminates the need for users to memorize numerous login credentials. It also benefits service providers because it enhances the security of their own platform, especially by eliminating the need to retain (often insecure and weak) passwords and eliminating forgotten password difficulties. SAML V1.0 became an OASIS standard in November 2002. SAML V1.1 followed in September 2003 and has seen significant success, gaining momentum in financial services, higher education, government, and other industry segments. SAML is an important standardized example of this new protocol class and will be widely used in business-to-business scenarios to reduce user-management costs. (Nongbri, Hadem, and Chettri 2018). SAML 2.0 Flow - source www.mutuallyhuman.com Even though SSO reduces the number of logins that are needed over heterogeneous environments, the risk that might be associated with the security of SSO might be detrimental if, for example, a Man-in-the Middle (MITM) attacker manages to gain control of the SSO credentials. The problem that a paper addresses is the lack of a proactive technique of hardening cloud-based SAML while combining SSO with a Multi-Factor Authentication (MFA) (Karie et al. 2020). with Redirect/POST Bindings and to the protocol which is distributed and used by Google’s partner companies to get full control over the authorization and authentication of hosted user accounts that can access web-based applications like Gmail or Google Calendar (Armando et al. 2008) SAML, however, provides a feature that OAuth2 does not: the SAML token contains user identification information (because of signing). With OAuth2, this is not provided by default; instead, the Resource Server must make an additional round trip to the Authorization Server to validate the token. 3.4.2.2 OAuth2 OAuth 2.0 (henceforth OAuth2) is a much more recent specification than SAML (circa late 2012). It has the advantage of being recent and takes into account the changes in the world during the past eight years. OAuth2 offers a simpler and more standardized solution that satisfies all of our existing requirements and eliminates the need for interoperability workarounds with native applications. OAuth 2.0 Flow - source www.mutuallyhuman.com OAuth 2.0 is one of the most extensively used authorization/single sign-on (SSO) protocols and serves as the foundation for the new SSO standard OpenID Connect. OAuth 2.0 is also popular for cloud-based online services and is used by many large companies, like Google, Facebook, and Microsoft, to allow restricted access to data. OAuth 2.0 has a flexible architecture and may be implemented and customized in a variety of ways based on the use case. Nevertheless, each of these implementation decisions and configuration options has its own security implications. Despite OAuth’s popularity, analysis attempts to date have focused primarily on discovering faults in individual implementations and have relied on formal models that abstract from numerous web aspects or have not provided a formal approach. four major attack types are still feasible if the security recommendations and certain best practices are followed, e.g. 307 redirect attack, IdP mix-up attack, state leak attack, and naive RP session integrity attack (Fett, Küsters, and Schmitz 2016). In addition to the current approach of basic authentication via client secret, it is recommended to add 2FA (Two Factor Authentication) as a feature for third-party client app authentication. (Singh and Chaudhary 2022) 3.4.3 Network Providing secure services requires the setup of cloud infrastructure security policies appropriately. After gaining access to this equipment, they are able to attack the host layer’s weaknesses and conduct malicious operations in this layer. All configurations should be recorded, maintained, and monitored under scheduled programs to prevent any error or misconfiguration which is fairly easy to achieve using the cloud services and infrastructure as code approach. Logging the events such as successful/unsuccessful access of the users to the equipment, configurations, and malicious traffics, and storing the logfiles on a separate server with limited access over a long period of time. Logging events such as successful/failed user access to equipment, configurations, and malicious traffic, and storing logfiles on a separate server with restricted access for an extended period of time (Jangjou and Sohrabi 2022). When it comes to networking, logical isolation makes a VPC environment inherently more secure. However, public cloud security isn’t automatic, even with VPC – it must be intentionally deployed. Cloud security is always a shared responsibility between a cloud provider and its clients. Virtual Private Cloud References "],["project-implementation-gcp.html", "Chapter 4 Project Implementation GCP 4.1 Shiny Application 4.2 Containerization 4.3 Shiny R Applications 4.4 Security Measures 4.5 Final result", " Chapter 4 Project Implementation GCP 4.1 Shiny Application Several shiny R applications were created for this project. The authentication part could be either shifted for the identity aware proxy or implemented in the application, as in example below. library(shiny) library(googleAuthR) gar_set_client(activate=&quot;web&quot;, web_json = &quot;client.json&quot;, scopes = &quot;https://www.googleapis.com/auth/webmasters&quot;) ui &lt;- fluidPage( titlePanel(&quot;Sample Google Sign-In&quot;), sidebarLayout( sidebarPanel( googleSignInUI(&quot;demo&quot;) ), mainPanel( h3(&quot;test&quot;), with(tags, dl(dt(&quot;Name&quot;), dd(textOutput(&quot;g_name&quot;)), dt(&quot;Email&quot;), dd(textOutput(&quot;g_email&quot;)), dt(&quot;Image&quot;), dd(uiOutput(&quot;g_image&quot;)) )) ) ) ) server &lt;- function(input, output, session) { sign_ins &lt;- shiny::callModule(googleSignIn, &quot;demo&quot;) output$g_name = renderText({ sign_ins()$name }) output$g_email = renderText({ sign_ins()$email }) output$g_image = renderUI({ img(src=sign_ins()$image) }) } shinyApp(ui = ui, server = server) While Shiny application could be hosted with the default config on any VM, websockets needs to be disabled for running shiny Google Cloud Run. disable_protocols websocket xdr-streaming xhr-streaming iframe-eventsource iframe-htmlfile xdr-polling iframe-xhr-polling; run_as shiny; server { listen 8080; location / { site_dir /srv/shiny-server; log_dir /var/log/shiny-server; directory_index off; } } 4.2 Containerization In order to dockerize shiny application, we will be using a docker image from rocker repository. Since the image is packaged with all the dependencies, we only need to add dependend packages (“googleAuthR”) and move required files to the container (shiny server configuration, client secret and the application itself). FROM rocker/shiny RUN apt-get update &amp;&amp; apt-get install -y \\ libcurl4-openssl-dev libssl-dev \\ &amp;&amp; apt-get clean \\ &amp;&amp; rm -rf /var/lib/apt/lists/ \\ &amp;&amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds RUN install2.r --error \\ -r &#39;http://cran.rstudio.com&#39; \\ googleAuthR COPY shiny-customized.config /etc/shiny-server/shiny-server.conf COPY client.json /srv/shiny-server/client.json COPY app.R /srv/shiny-server/app.R RUN chmod -R 775 /srv/shiny-server/ EXPOSE 8080 USER shiny CMD [&quot;/usr/bin/shiny-server&quot;] In order to build a docker container and push the image to the Google Container registry, these commands are utilized. It is possible to automate the pipeline and build container from source (e.g. project github repository), but this feature was outside the scope of the project. gcloud auth login --project {project id} PROJECTID=$(gcloud config get-value project) docker build . -t gcr.io/$PROJECTID/signin docker push gcr.io/$PROJECTID/signin For debugging purposes, application can be launched locally: docker run --rm -p 8080:8080 gcr.io/$PROJECTID/simpleauth:latest 4.2.1 Terraform Since we are following Infrastructure as Code pattern, the whole required infrastructure could be defined in a terraform file. We need to select project, region, service and add application deployment config. In case of Cloud Run deployment, terraform file can be defined like this: cloudrun-shiny.tf provider &quot;google&quot;{ project = &quot;shiny-cloud-project&quot; region = &quot;europe-west1&quot; } resource &quot;google_project_service&quot; &quot;run&quot; { service = &quot;run.googleapis.com&quot; } resource &quot;google_cloud_run_service&quot; &quot;shiny-simple-signin&quot; { name = &quot;shiny-simple-signin&quot; location = &quot;europe-west1&quot; template { spec { containers { image = &quot;gcr.io/shiny-cloud-project/signin:latest&quot; resources { limits = { cpu = &quot;1000m&quot; memory = &quot;1024Mi&quot; } } ports { container_port = 8080 } } container_concurrency = 80 timeout_seconds = 300 } metadata { annotations = { &quot;autoscaling.knative.dev/minScale&quot; = 0 &quot;autoscaling.knative.dev/maxScale&quot; = 1 } } } traffic { percent = 100 latest_revision = true } depends_on = [google_project_service.run] } resource &quot;google_cloud_run_service_iam_member&quot; &quot;allUsers&quot; { service = google_cloud_run_service.shiny-simple-signin.name location = google_cloud_run_service.shiny-simple-signin.location role = &quot;roles/run.invoker&quot; member = &quot;allUsers&quot; } It is convenient to print shiny app URL after the deployment, thus output value is added in outputs.tf: output &quot;url&quot; { value = google_cloud_run_service.shiny-simple-signin.status[0].url } Terraform is initiated using the init command, plan command will generate the plan and apply command will push the changes of the infrastructure. After that terraform will hold state regarding your infrastructure and configuration management. Terraform uses this state to map real-world resources to the configuration, keep track of metadata, and optimize efficiency for huge infrastructures. While it is preferred to store the state centrally (e.g. GCS bucket), this was also outside of the project scope. terraform init terraform plan terraform apply 4.2.2 Cloud Run Cloud Run is a managed compute platform that supports the execution of containers invoked by requests or events. Since Cloud Run is serverless, it abstracts the infrastructure administration. Additionally, the serverless approach provides more fine-grained billing and can significantly reduce the cost (e.g. in case the application is not in use). With a manually created GKE cluster, the nodes and environment are always on which means that you are billed for them regardless of utilization. With Cloud Run, the service is merely available and the billing is done only for the actual consumption. In our case, we have deployed two services, which are not exposed to the internet. The only traffic allow is within the project or via Load balancer. Authentication for these services are handled either via Identity Aware Proxy or within the application. Cloud Run Withing the deployed service, all the necessary metrics are tracked for enhanced monitoring. Minimum instances could be set to 0 to make the application offline when it is not in use. Cloud Run 4.2.3 GKE Google Kubernetes Engine is a straightforward way of setting up a Kubernetes Cluster in Cloud. These clusters are fully managed by Google Site Reliability Engineers and Google ensures that your cluster is available and up-to-date. It also supports the common Docker container format and runs on Container-Optimized OS. Once the Cluster is created, the deployment is similar to Cloud Run. It can be done via Google Cloud Console, GCP SDK, or integrated into any CI/CD pipeline using Jenkins, GitActions, or similar tools. To enhance the security, private GKE cluster is deployed. The service is exposed via node port internally, which is later mapped as a back-end service in the load balancer configuration. This way external connectivity is only possible via Load Balancer. App deployed in GKE Since Shiny Server is a stateful application, session affinity is also needed in case there are more than one active pods (which are also referred to as sticky sessions). GKE + Load balancer 4.3 Shiny R Applications A simple app with a login module. This module is appropriate if it is not needed to authenticate APIs in the application and only require a login form. Shiny - Google Sign In This uses the most modern gar_shiny_* family of functions to create authentication. library(shiny) library(googleAuthR) gar_set_client(activate=&quot;web&quot;, web_json = &quot;client.json&quot;, scopes = &quot;https://www.googleapis.com/auth/webmasters&quot;) options(googleAuthR.redirect = &#39;https://www.shinycloud.online&#39;) ui &lt;- fluidPage( titlePanel(&quot;Old Faithful Geyser Data&quot;), sidebarLayout( sidebarPanel( # numericInput(&quot;bins&quot;, value = 2, label = &quot;bins&quot;) sliderInput(&quot;bins&quot;, &quot;Number of bins:&quot;, min = 1, max = 50, value = 30) ), mainPanel( plotOutput(&quot;distPlot&quot;) ) ) ) ## server.R server &lt;- function(input, output, session){ gar_shiny_auth(session) output$distPlot &lt;- renderPlot({ x &lt;- faithful[, 2] bins &lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = &#39;darkgray&#39;, border = &#39;white&#39;) }) } shinyApp(gar_shiny_ui(ui), server) After the authentication is completed (in this case it is possible to adjust scope, e.g. users can allow access to their analytics API or Google Drive), user will be redirected to the app. In some platforms (such as shinyapps.io or a Kubernetes cluster, as in this case), the URL you are authenticating from will not match the URL of the Docker container the script is running in. In this instance, it must be manually set in the application. Shiny - Google Oauth2 4.4 Security Measures 4.4.1 Load balancing Load balancers are managed services on GCP that distribute application traffic across many instances. An in previous example, load balancer is responsible for routing the requests from *.shinycloud.online to the internal backend endpoints (Cloud Run, GKE cluster node port). Load Balancer 4.4.2 DNS In order to access the application from external network, it is wise to use DNS and enable the connectivity only over HTTPS protocol. This way users can trust that they are reaching correct website and data is protected via TLS encryption. DNS instance can be created using the following command: gcloud beta dns --project=$PROJECTID managed-zones create shinycloud-online --description=&quot;&quot; --dns-name=&quot;shinycloud.online.&quot; --visibility=&quot;public&quot; --dnssec-state=&quot;off&quot; --log-dns-queries After that, address (A) record is mapped to the load balancer while the Canonical Name name is added as shinycloud.online. It is possible to create subdomains (e.g. login.shinycloud.online) and map them to separate/standalone shiny applications as well. Cloud DNS 4.4.3 Certificates Certificates signed by CA 4.4.4 Identity aware proxy Identity-Aware Proxy (IAP) is a Google Cloud Platform service that intercepts web requests sent to your application, authenticates the user making the request using the Google Identity Service, and only allows the request to proceed if it originates from a user which is authorized. In the following example, IAP is enabled for 2 backend services out of three via the Load Balancer. This means that user will need to authorize via IAP proxy prior to reaching the app itself. Identity aware proxy In case the user does not have the required permissions, he will be presented with the error screen. Identity aware proxy - not authorized 4.4.5 Audit Logs are ingested from Google Cloud services and stored securely with no configuration necessary. GKE workload logs are captured automatically, while VM workload logs are captured via the Ops Agent. Having all the logs stored centrally is convenient for the analytics/anomaly detection purposes discussed in previous chapters. Cloud Logging 4.5 Final result Shiny Analytics Hub - Architecture https://shinycloud.online/ - main public app (only Google Sign In) https://login.shinycloud.online/ - private app (Google OAuth + identity aware proxy) https://gke.shinycloud.online/ - scalable app (identity aware proxy only) https://_____.shinycloud.online/ - any new app, depending on the security and scalability requirements All apps will be terminated on 2022-06-20 "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
